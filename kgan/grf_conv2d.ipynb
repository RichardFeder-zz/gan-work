{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/apps/intel17/python/2.7.13/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['shuffle']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "import keras.activations as ka\n",
    "import keras.layers.advanced_activations as kla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8931808852505277522, name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 18251848448588341857\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 6924743809988139107\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10913290650\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 1\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 95556148136808875\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\", name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10913290650\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 11222102849778639394\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\", name: \"/device:GPU:2\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10913290650\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 3\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 4476369411335042486\n",
       " physical_device_desc: \"device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\", name: \"/device:GPU:3\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10913290650\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 2\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 1165223719548903242\n",
       " physical_device_desc: \"device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clayer(nfeatures, kernel_size=(3, 3), strides=(1,1), use_bias=True, **kwargs):\n",
    "    \n",
    "    return kl.Conv2D(nfeatures,\n",
    "                     kernel_size=kernel_size,strides=strides, \n",
    "                     use_bias=use_bias, padding='same', **kwargs)\n",
    "\n",
    "def NonLinearity():\n",
    "    \n",
    "    #return kl.Activation('relu')\n",
    "    return kla.LeakyReLU(alpha=0.1)\n",
    "\n",
    "def ResBlock(x, nf, dropout=None, **kwargs):\n",
    "    \n",
    "    shortcut = x\n",
    "    x = clayer(nf, **kwargs)(x)\n",
    "    x = kl.BatchNormalization(center=False)(x)\n",
    "    x = NonLinearity()(x)\n",
    "    if dropout is not None:\n",
    "        x = kl.Dropout(dropout)(x)\n",
    "    x = clayer(nf, **kwargs)(x)\n",
    "    x = kl.Add()([x, shortcut])\n",
    "    x = kl.BatchNormalization(center=False)(x)\n",
    "    x = NonLinearity()(x)\n",
    "    if dropout is not None:\n",
    "        x = kl.Dropout(dropout)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def DownBlock(x, nf, dropout=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This reduces the dimensionality using convolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    shortcut = clayer(nf, kernel_size=(1,1), strides=(2,2), **kwargs)(x)\n",
    "    x = clayer(nf, kernel_size=(2,2), strides=(2,2), **kwargs)(x)\n",
    "    x = kl.BatchNormalization(center=False)(x)\n",
    "    x = NonLinearity()(x)\n",
    "    if dropout is not None:\n",
    "        x = kl.Dropout(dropout)(x)\n",
    "    x = clayer(nf, **kwargs)(x)\n",
    "    x = kl.Add()([x, shortcut])\n",
    "    x = kl.BatchNormalization(center=False)(x)\n",
    "    x = NonLinearity()(x)\n",
    "    if dropout is not None:\n",
    "        x = kl.Dropout(dropout)(x)\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "def get_discriminator(input_shape, nclasses=1, nlevels=4, base_features=32, dropout=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a 2D CNN based on the ResNet model.\n",
    "    Parameters\n",
    "    ---------\n",
    "    input_shape : np.array((3))\n",
    "        Specifies the size of the input data, usually (weight, height, 1)\n",
    "    \n",
    "    nlevels : int (optional)\n",
    "        The number of times (-1) to downsample the resolution\n",
    "        Default 4.\n",
    "    base_features: int (optional)\n",
    "        The number of features the input layer should learn.\n",
    "        Default 64.\n",
    "    Returns\n",
    "    -------\n",
    "    model :\n",
    "        The keras model.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_state = kl.Input(shape=input_shape)\n",
    "\n",
    "    #Input layer\n",
    "    x = kl.Conv2D(base_features, (7,7), use_bias=False, padding='same', **kwargs)(input_state)\n",
    "    #x = kl.BatchNormalization(center=False)(x)\n",
    "    #x = NonLinearity()(x)\n",
    "\n",
    "    for level in range(nlevels):\n",
    "        \n",
    "        if level != 0:\n",
    "            # This layer ups the number of features while downsampling spatially\n",
    "            x = DownBlock(x, base_features*2**level, dropout=dropout, **kwargs)\n",
    "    \n",
    "        x = ResBlock(x, base_features*2**level, dropout=dropout, **kwargs)\n",
    "        x = ResBlock(x, base_features*2**level, dropout=dropout, **kwargs)\n",
    "        #if level in [1, 2]:\n",
    "        #    x = ResBlock(x, base_features*2**level, dropout=dropout, **kwargs)\n",
    "\n",
    "    x = kl.GlobalAvgPool2D()(x)\n",
    "    #x = kl.Dense(units=256, activation='relu')(x)\n",
    "    #x = kl.Dropout(0.5)(x)\n",
    "    \n",
    "    #Sigmoid goes with binary crossentropy\n",
    "    x = kl.Dense(units=nclasses, activation='sigmoid')(x)\n",
    "\n",
    "    model = km.Model(inputs = input_state, outputs = x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 32)   1568        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 28, 28, 32)   9248        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 28, 28, 32)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 28, 28, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 28, 28, 32)   9248        leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 28, 28, 32)   0           conv2d_16[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 28, 28, 32)   96          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 28, 28, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 28, 28, 32)   9248        leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 28, 28, 32)   96          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 28, 28, 32)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 28, 28, 32)   9248        leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 28, 28, 32)   0           conv2d_18[0][0]                  \n",
      "                                                                 leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 28, 28, 32)   96          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 28, 28, 32)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 14, 14, 64)   8256        leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 64)   2112        leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 64)   0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 14, 14, 64)   192         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 64)   0           conv2d_23[0][0]                  \n",
      "                                                                 leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 14, 14, 64)   192         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 64)   192         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 64)   36928       leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 64)   0           conv2d_25[0][0]                  \n",
      "                                                                 leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 64)   192         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 14, 14, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 7, 7, 128)    32896       leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 7, 7, 128)    384         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 7, 7, 128)    147584      leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 7, 7, 128)    8320        leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 7, 7, 128)    0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 7, 7, 128)    384         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 128)    147584      leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 7, 7, 128)    384         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 128)    147584      leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 128)    0           conv2d_30[0][0]                  \n",
      "                                                                 leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 7, 7, 128)    384         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 128)    147584      leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 128)    384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 128)    147584      leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 128)    0           conv2d_32[0][0]                  \n",
      "                                                                 leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 7, 7, 128)    384         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 7, 7, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 128)          0           leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,016,673\n",
      "Trainable params: 1,014,113\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc = get_discriminator(input_shape=img_shape, nlevels=3)\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Generator which takes noise as input\n",
    "#model.add(Dense(256, input_dim=latent_dim))\n",
    "\n",
    "def UpBlock(x, nf, dropout=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This increases the dimensionality using transposed convolutions.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = kl.Conv2DTranspose(nf, kernel_size=(3, 3), strides=(2,2), padding='same')(x)\n",
    "    x = kl.BatchNormalization(center=False)(x)\n",
    "    x = NonLinearity()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_generator(input_shape, image_shape, nlevels=4, base_features=32, dropout=None, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in noise and returns an image\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the input and reshape it into an image of the right shape.\n",
    "    input_state = kl.Input(shape=input_shape)\n",
    "    \n",
    "    ii_width = image_shape[0]//2**nlevels\n",
    "    ii_height = image_shape[1]//2**nlevels\n",
    "    \n",
    "    x = kl.Dense(units=ii_width*ii_height*base_features)(input_state)\n",
    "    x = NonLinearity()(x)\n",
    "    x = kl.Reshape((ii_width, ii_height, base_features))(x)\n",
    "\n",
    "    for level in range(nlevels):\n",
    "        \n",
    "        #Reduce nfeatures by half\n",
    "        x = UpBlock(x, base_features//2**(level+1), dropout=dropout, **kwargs)\n",
    "    \n",
    "        x = ResBlock(x, base_features//2**(level+1), dropout=dropout, **kwargs)\n",
    "        x = ResBlock(x, base_features//2**(level+1), dropout=dropout, **kwargs)\n",
    "        \n",
    "    # This last layer is linear\n",
    "    x = kl.Conv2D(1, kernel_size=(7,7), strides=(1,1), padding='same')(x)\n",
    "    x = kl.Reshape((image_shape[0], image_shape[1], 1))(x)\n",
    "\n",
    "    model = km.Model(inputs = input_state, outputs = x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2048)         206848      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 8, 8, 32)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 16, 16, 16)   4624        reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 16)   48          conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 16)   48          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 16)   0           conv2d_34[0][0]                  \n",
      "                                                                 leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 16)   48          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 16)   48          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 16)   0           conv2d_36[0][0]                  \n",
      "                                                                 leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 16)   48          add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 32, 32, 8)    1160        leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 8)    24          conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 8)    584         leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 32, 32, 8)    24          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 8)    584         leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 32, 8)    0           conv2d_38[0][0]                  \n",
      "                                                                 leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 8)    24          add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 32, 32, 8)    584         leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 8)    24          conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 32, 32, 8)    584         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 32, 32, 8)    0           conv2d_40[0][0]                  \n",
      "                                                                 leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 8)    24          add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 64, 64, 4)    292         leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 64, 4)    12          conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 64, 64, 4)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 64, 4)    148         leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 64, 4)    12          conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 64, 64, 4)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 64, 4)    148         leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 64, 64, 4)    0           conv2d_42[0][0]                  \n",
      "                                                                 leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 64, 4)    12          add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 64, 64, 4)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 64, 4)    148         leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 64, 4)    12          conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 64, 64, 4)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 64, 4)    148         leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 64, 4)    0           conv2d_44[0][0]                  \n",
      "                                                                 leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 64, 4)    12          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 64, 64, 4)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 64, 1)    197         leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 64, 64)       0           conv2d_45[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 225,749\n",
      "Trainable params: 225,469\n",
      "Non-trainable params: 280\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen = get_generator(input_shape=(latent_dim,), image_shape=(img_rows, img_cols), nlevels=3)\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=None\n",
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some metadata\n",
    "img_rows = 64\n",
    "img_cols = 64\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok let's build and compile everything\n",
    "optimizer = Adam(1e-5, 0.99, 0.999)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = get_discriminator(input_shape=img_shape, nlevels=3)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = get_generator(input_shape=(latent_dim,), image_shape=(img_rows, img_cols), nlevels=3)\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "validity = discriminator(img)\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains the generator to fool the discriminator\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grf_2d(A, alpha, image_shape=(64, 64), seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed=seed)\n",
    "        \n",
    "    grf = np.random.normal(size=image_shape)\n",
    "    \n",
    "    kx = np.fft.fftfreq(image_shape[0])\n",
    "    ky = np.fft.fftfreq(image_shape[1])\n",
    "    kk = np.sqrt(kx[:, np.newaxis]**2 + ky[np.newaxis, :]**2)\n",
    "    \n",
    "    kk[0, 0] = A\n",
    "    pk = A*kk**alpha\n",
    "    \n",
    "    return np.fft.ifft2(np.fft.fft2(grf)*np.sqrt(pk)).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "grfe = grf_2d(1.0, alpha=-2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAECCAYAAADw0Rw8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b9cf47de250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kx = np.fft.fftfreq(64, d=0.1)\n",
    "ky = np.fft.fftfreq(64, d=0.1)\n",
    "kk = np.sqrt(kx[:, np.newaxis]**2 + ky[np.newaxis, :]**2)\n",
    "\n",
    "plot(kk.ravel(), abs(np.fft.fft2(grfe)).ravel(), '.', alpha=0.2)\n",
    "yscale('log')\n",
    "xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.712320, acc.: 50.00%] [G loss: 0.839478]\n",
      "1 [D loss: 0.716504, acc.: 50.00%] [G loss: 0.838611]\n",
      "2 [D loss: 0.719919, acc.: 50.00%] [G loss: 0.838777]\n",
      "3 [D loss: 0.717599, acc.: 50.00%] [G loss: 0.833510]\n",
      "4 [D loss: 0.720352, acc.: 50.00%] [G loss: 0.834288]\n",
      "5 [D loss: 0.717254, acc.: 50.00%] [G loss: 0.839686]\n",
      "6 [D loss: 0.713833, acc.: 50.00%] [G loss: 0.840525]\n",
      "7 [D loss: 0.712296, acc.: 50.00%] [G loss: 0.846019]\n",
      "8 [D loss: 0.709266, acc.: 50.00%] [G loss: 0.841528]\n",
      "9 [D loss: 0.713351, acc.: 50.00%] [G loss: 0.841860]\n",
      "10 [D loss: 0.711138, acc.: 50.00%] [G loss: 0.846202]\n",
      "11 [D loss: 0.711226, acc.: 50.00%] [G loss: 0.848123]\n",
      "12 [D loss: 0.710587, acc.: 50.00%] [G loss: 0.849694]\n",
      "13 [D loss: 0.710811, acc.: 50.00%] [G loss: 0.851243]\n",
      "14 [D loss: 0.708665, acc.: 50.00%] [G loss: 0.853492]\n",
      "15 [D loss: 0.709431, acc.: 50.00%] [G loss: 0.855076]\n",
      "16 [D loss: 0.707045, acc.: 50.00%] [G loss: 0.859821]\n",
      "17 [D loss: 0.705882, acc.: 50.00%] [G loss: 0.860649]\n",
      "18 [D loss: 0.704090, acc.: 48.44%] [G loss: 0.862860]\n",
      "19 [D loss: 0.704641, acc.: 50.00%] [G loss: 0.860952]\n",
      "20 [D loss: 0.703222, acc.: 50.00%] [G loss: 0.863179]\n",
      "21 [D loss: 0.701899, acc.: 50.00%] [G loss: 0.860202]\n",
      "22 [D loss: 0.699800, acc.: 50.00%] [G loss: 0.865529]\n",
      "23 [D loss: 0.698145, acc.: 48.44%] [G loss: 0.867361]\n",
      "24 [D loss: 0.699207, acc.: 48.44%] [G loss: 0.873156]\n",
      "25 [D loss: 0.699610, acc.: 50.00%] [G loss: 0.872222]\n",
      "26 [D loss: 0.700811, acc.: 50.00%] [G loss: 0.872289]\n",
      "27 [D loss: 0.695459, acc.: 50.00%] [G loss: 0.876072]\n",
      "28 [D loss: 0.694763, acc.: 48.44%] [G loss: 0.879498]\n",
      "29 [D loss: 0.696926, acc.: 50.00%] [G loss: 0.878048]\n",
      "30 [D loss: 0.697147, acc.: 50.00%] [G loss: 0.880925]\n",
      "31 [D loss: 0.696876, acc.: 50.00%] [G loss: 0.882812]\n",
      "32 [D loss: 0.691865, acc.: 50.00%] [G loss: 0.891118]\n",
      "33 [D loss: 0.695189, acc.: 50.00%] [G loss: 0.882687]\n",
      "34 [D loss: 0.693435, acc.: 50.00%] [G loss: 0.884522]\n",
      "35 [D loss: 0.693193, acc.: 50.00%] [G loss: 0.890312]\n",
      "36 [D loss: 0.687349, acc.: 50.00%] [G loss: 0.896420]\n",
      "37 [D loss: 0.688927, acc.: 50.00%] [G loss: 0.899110]\n",
      "38 [D loss: 0.688256, acc.: 50.00%] [G loss: 0.893255]\n",
      "39 [D loss: 0.687609, acc.: 50.00%] [G loss: 0.896495]\n",
      "40 [D loss: 0.687695, acc.: 50.00%] [G loss: 0.903614]\n",
      "41 [D loss: 0.683020, acc.: 50.00%] [G loss: 0.893934]\n",
      "42 [D loss: 0.690084, acc.: 50.00%] [G loss: 0.905392]\n",
      "43 [D loss: 0.681494, acc.: 50.00%] [G loss: 0.907322]\n",
      "44 [D loss: 0.683383, acc.: 50.00%] [G loss: 0.906032]\n",
      "45 [D loss: 0.680357, acc.: 50.00%] [G loss: 0.911272]\n",
      "46 [D loss: 0.683532, acc.: 50.00%] [G loss: 0.906527]\n",
      "47 [D loss: 0.678353, acc.: 50.00%] [G loss: 0.910248]\n",
      "48 [D loss: 0.677713, acc.: 50.00%] [G loss: 0.913624]\n",
      "49 [D loss: 0.678326, acc.: 50.00%] [G loss: 0.921611]\n",
      "50 [D loss: 0.678998, acc.: 50.00%] [G loss: 0.916916]\n",
      "51 [D loss: 0.678282, acc.: 50.00%] [G loss: 0.924623]\n",
      "52 [D loss: 0.678403, acc.: 50.00%] [G loss: 0.915278]\n",
      "53 [D loss: 0.678577, acc.: 50.00%] [G loss: 0.925166]\n",
      "54 [D loss: 0.673885, acc.: 50.00%] [G loss: 0.924256]\n",
      "55 [D loss: 0.674642, acc.: 50.00%] [G loss: 0.930455]\n",
      "56 [D loss: 0.668743, acc.: 50.00%] [G loss: 0.925876]\n",
      "57 [D loss: 0.664699, acc.: 50.00%] [G loss: 0.925088]\n",
      "58 [D loss: 0.673119, acc.: 50.00%] [G loss: 0.930681]\n",
      "59 [D loss: 0.666261, acc.: 50.00%] [G loss: 0.931320]\n",
      "60 [D loss: 0.663755, acc.: 50.00%] [G loss: 0.928414]\n",
      "61 [D loss: 0.665607, acc.: 50.00%] [G loss: 0.930808]\n",
      "62 [D loss: 0.656423, acc.: 53.12%] [G loss: 0.934664]\n",
      "63 [D loss: 0.659538, acc.: 51.56%] [G loss: 0.936863]\n",
      "64 [D loss: 0.668433, acc.: 51.56%] [G loss: 0.943986]\n",
      "65 [D loss: 0.664467, acc.: 50.00%] [G loss: 0.944257]\n",
      "66 [D loss: 0.665043, acc.: 51.56%] [G loss: 0.946698]\n",
      "67 [D loss: 0.655550, acc.: 50.00%] [G loss: 0.938078]\n",
      "68 [D loss: 0.655641, acc.: 53.12%] [G loss: 0.947955]\n",
      "69 [D loss: 0.658613, acc.: 51.56%] [G loss: 0.951678]\n",
      "70 [D loss: 0.655090, acc.: 51.56%] [G loss: 0.954758]\n",
      "71 [D loss: 0.658497, acc.: 54.69%] [G loss: 0.953745]\n",
      "72 [D loss: 0.660076, acc.: 51.56%] [G loss: 0.954602]\n",
      "73 [D loss: 0.655029, acc.: 50.00%] [G loss: 0.961312]\n",
      "74 [D loss: 0.651410, acc.: 54.69%] [G loss: 0.956595]\n",
      "75 [D loss: 0.653992, acc.: 53.12%] [G loss: 0.958543]\n",
      "76 [D loss: 0.646796, acc.: 53.12%] [G loss: 0.968775]\n",
      "77 [D loss: 0.649297, acc.: 51.56%] [G loss: 0.973531]\n",
      "78 [D loss: 0.648070, acc.: 51.56%] [G loss: 0.971331]\n",
      "79 [D loss: 0.643425, acc.: 57.81%] [G loss: 0.966629]\n",
      "80 [D loss: 0.643562, acc.: 56.25%] [G loss: 0.965693]\n",
      "81 [D loss: 0.644328, acc.: 57.81%] [G loss: 0.980097]\n",
      "82 [D loss: 0.648486, acc.: 56.25%] [G loss: 0.972152]\n",
      "83 [D loss: 0.638124, acc.: 56.25%] [G loss: 0.981183]\n",
      "84 [D loss: 0.639424, acc.: 54.69%] [G loss: 0.989634]\n",
      "85 [D loss: 0.641801, acc.: 54.69%] [G loss: 0.980189]\n",
      "86 [D loss: 0.641672, acc.: 54.69%] [G loss: 0.978783]\n",
      "87 [D loss: 0.636114, acc.: 54.69%] [G loss: 0.982987]\n",
      "88 [D loss: 0.638977, acc.: 56.25%] [G loss: 0.991638]\n",
      "89 [D loss: 0.637696, acc.: 54.69%] [G loss: 0.987996]\n",
      "90 [D loss: 0.634244, acc.: 56.25%] [G loss: 0.998752]\n",
      "91 [D loss: 0.633554, acc.: 57.81%] [G loss: 0.996616]\n",
      "92 [D loss: 0.632806, acc.: 56.25%] [G loss: 0.995496]\n",
      "93 [D loss: 0.637961, acc.: 57.81%] [G loss: 0.990052]\n",
      "94 [D loss: 0.633797, acc.: 57.81%] [G loss: 1.004661]\n",
      "95 [D loss: 0.635280, acc.: 54.69%] [G loss: 1.005240]\n",
      "96 [D loss: 0.628167, acc.: 57.81%] [G loss: 1.023059]\n",
      "97 [D loss: 0.638823, acc.: 57.81%] [G loss: 0.994038]\n",
      "98 [D loss: 0.629410, acc.: 54.69%] [G loss: 1.013332]\n",
      "99 [D loss: 0.630116, acc.: 59.38%] [G loss: 1.003150]\n",
      "100 [D loss: 0.623944, acc.: 59.38%] [G loss: 1.004741]\n",
      "101 [D loss: 0.619679, acc.: 62.50%] [G loss: 1.016751]\n",
      "102 [D loss: 0.614929, acc.: 59.38%] [G loss: 1.020994]\n",
      "103 [D loss: 0.614011, acc.: 70.31%] [G loss: 0.996831]\n",
      "104 [D loss: 0.617991, acc.: 60.94%] [G loss: 1.029217]\n",
      "105 [D loss: 0.625647, acc.: 60.94%] [G loss: 1.009780]\n",
      "106 [D loss: 0.609197, acc.: 64.06%] [G loss: 1.015816]\n",
      "107 [D loss: 0.611058, acc.: 65.62%] [G loss: 1.017704]\n",
      "108 [D loss: 0.620040, acc.: 59.38%] [G loss: 1.024594]\n",
      "109 [D loss: 0.614261, acc.: 64.06%] [G loss: 1.019264]\n",
      "110 [D loss: 0.599860, acc.: 67.19%] [G loss: 1.016817]\n",
      "111 [D loss: 0.607820, acc.: 68.75%] [G loss: 1.007921]\n",
      "112 [D loss: 0.598460, acc.: 71.88%] [G loss: 1.031249]\n",
      "113 [D loss: 0.602449, acc.: 70.31%] [G loss: 1.026922]\n",
      "114 [D loss: 0.612196, acc.: 64.06%] [G loss: 1.020555]\n",
      "115 [D loss: 0.598772, acc.: 68.75%] [G loss: 1.034597]\n",
      "116 [D loss: 0.606018, acc.: 68.75%] [G loss: 1.046898]\n",
      "117 [D loss: 0.603145, acc.: 67.19%] [G loss: 1.055469]\n",
      "118 [D loss: 0.592732, acc.: 73.44%] [G loss: 1.021527]\n",
      "119 [D loss: 0.608656, acc.: 62.50%] [G loss: 1.022818]\n",
      "120 [D loss: 0.591604, acc.: 79.69%] [G loss: 1.030089]\n",
      "121 [D loss: 0.598232, acc.: 71.88%] [G loss: 1.036805]\n",
      "122 [D loss: 0.597512, acc.: 73.44%] [G loss: 1.034744]\n",
      "123 [D loss: 0.576329, acc.: 76.56%] [G loss: 1.039488]\n",
      "124 [D loss: 0.583116, acc.: 78.12%] [G loss: 1.043097]\n",
      "125 [D loss: 0.590845, acc.: 75.00%] [G loss: 1.036916]\n",
      "126 [D loss: 0.593266, acc.: 70.31%] [G loss: 1.047059]\n",
      "127 [D loss: 0.593118, acc.: 68.75%] [G loss: 1.051993]\n",
      "128 [D loss: 0.598511, acc.: 75.00%] [G loss: 1.058773]\n",
      "129 [D loss: 0.579827, acc.: 81.25%] [G loss: 1.043288]\n",
      "130 [D loss: 0.590242, acc.: 71.88%] [G loss: 1.041012]\n",
      "131 [D loss: 0.588902, acc.: 76.56%] [G loss: 1.068111]\n",
      "132 [D loss: 0.580848, acc.: 76.56%] [G loss: 1.052964]\n",
      "133 [D loss: 0.580000, acc.: 78.12%] [G loss: 1.045365]\n",
      "134 [D loss: 0.583308, acc.: 75.00%] [G loss: 1.068610]\n",
      "135 [D loss: 0.577317, acc.: 76.56%] [G loss: 1.043677]\n",
      "136 [D loss: 0.589119, acc.: 75.00%] [G loss: 1.092922]\n",
      "137 [D loss: 0.573594, acc.: 76.56%] [G loss: 1.058020]\n",
      "138 [D loss: 0.570459, acc.: 78.12%] [G loss: 1.074070]\n",
      "139 [D loss: 0.558374, acc.: 75.00%] [G loss: 1.090653]\n",
      "140 [D loss: 0.570583, acc.: 76.56%] [G loss: 1.067208]\n",
      "141 [D loss: 0.548342, acc.: 84.38%] [G loss: 1.072023]\n",
      "142 [D loss: 0.558668, acc.: 78.12%] [G loss: 1.095948]\n",
      "143 [D loss: 0.561237, acc.: 75.00%] [G loss: 1.069400]\n",
      "144 [D loss: 0.571206, acc.: 79.69%] [G loss: 1.053617]\n",
      "145 [D loss: 0.566148, acc.: 82.81%] [G loss: 1.067677]\n",
      "146 [D loss: 0.543746, acc.: 79.69%] [G loss: 1.072832]\n",
      "147 [D loss: 0.560099, acc.: 82.81%] [G loss: 1.070039]\n",
      "148 [D loss: 0.560891, acc.: 78.12%] [G loss: 1.085005]\n",
      "149 [D loss: 0.562660, acc.: 84.38%] [G loss: 1.059863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.538205, acc.: 89.06%] [G loss: 1.080077]\n",
      "151 [D loss: 0.547544, acc.: 84.38%] [G loss: 1.062322]\n",
      "152 [D loss: 0.555958, acc.: 82.81%] [G loss: 1.075511]\n",
      "153 [D loss: 0.547715, acc.: 78.12%] [G loss: 1.030094]\n",
      "154 [D loss: 0.543484, acc.: 81.25%] [G loss: 1.103002]\n",
      "155 [D loss: 0.533879, acc.: 85.94%] [G loss: 1.104216]\n",
      "156 [D loss: 0.536476, acc.: 84.38%] [G loss: 1.083453]\n",
      "157 [D loss: 0.539363, acc.: 84.38%] [G loss: 1.105840]\n",
      "158 [D loss: 0.521695, acc.: 85.94%] [G loss: 1.096212]\n",
      "159 [D loss: 0.531684, acc.: 85.94%] [G loss: 1.097392]\n",
      "160 [D loss: 0.522319, acc.: 89.06%] [G loss: 1.082623]\n",
      "161 [D loss: 0.530965, acc.: 84.38%] [G loss: 1.095212]\n",
      "162 [D loss: 0.548426, acc.: 82.81%] [G loss: 1.118486]\n",
      "163 [D loss: 0.524500, acc.: 84.38%] [G loss: 1.088098]\n",
      "164 [D loss: 0.528993, acc.: 84.38%] [G loss: 1.133167]\n",
      "165 [D loss: 0.510415, acc.: 85.94%] [G loss: 1.099617]\n",
      "166 [D loss: 0.526919, acc.: 85.94%] [G loss: 1.098171]\n",
      "167 [D loss: 0.519369, acc.: 85.94%] [G loss: 1.121234]\n",
      "168 [D loss: 0.519286, acc.: 81.25%] [G loss: 1.104477]\n",
      "169 [D loss: 0.530249, acc.: 87.50%] [G loss: 1.103345]\n",
      "170 [D loss: 0.518253, acc.: 87.50%] [G loss: 1.109884]\n",
      "171 [D loss: 0.531335, acc.: 81.25%] [G loss: 1.150806]\n",
      "172 [D loss: 0.524282, acc.: 84.38%] [G loss: 1.133168]\n",
      "173 [D loss: 0.508562, acc.: 87.50%] [G loss: 1.131790]\n",
      "174 [D loss: 0.497943, acc.: 89.06%] [G loss: 1.098279]\n",
      "175 [D loss: 0.510751, acc.: 82.81%] [G loss: 1.131394]\n",
      "176 [D loss: 0.495346, acc.: 90.62%] [G loss: 1.112355]\n",
      "177 [D loss: 0.519039, acc.: 85.94%] [G loss: 1.127787]\n",
      "178 [D loss: 0.492327, acc.: 90.62%] [G loss: 1.148394]\n",
      "179 [D loss: 0.508106, acc.: 82.81%] [G loss: 1.137222]\n",
      "180 [D loss: 0.501984, acc.: 87.50%] [G loss: 1.158682]\n",
      "181 [D loss: 0.511526, acc.: 89.06%] [G loss: 1.149783]\n",
      "182 [D loss: 0.497959, acc.: 85.94%] [G loss: 1.129280]\n",
      "183 [D loss: 0.491627, acc.: 90.62%] [G loss: 1.120484]\n",
      "184 [D loss: 0.500379, acc.: 90.62%] [G loss: 1.145723]\n",
      "185 [D loss: 0.484059, acc.: 92.19%] [G loss: 1.161409]\n",
      "186 [D loss: 0.487663, acc.: 90.62%] [G loss: 1.121269]\n",
      "187 [D loss: 0.498547, acc.: 90.62%] [G loss: 1.152488]\n",
      "188 [D loss: 0.482857, acc.: 93.75%] [G loss: 1.199973]\n",
      "189 [D loss: 0.495364, acc.: 90.62%] [G loss: 1.168680]\n",
      "190 [D loss: 0.484967, acc.: 87.50%] [G loss: 1.163817]\n",
      "191 [D loss: 0.504487, acc.: 90.62%] [G loss: 1.123799]\n",
      "192 [D loss: 0.486665, acc.: 90.62%] [G loss: 1.126642]\n",
      "193 [D loss: 0.473723, acc.: 90.62%] [G loss: 1.178290]\n",
      "194 [D loss: 0.516667, acc.: 85.94%] [G loss: 1.186770]\n",
      "195 [D loss: 0.457684, acc.: 89.06%] [G loss: 1.204255]\n",
      "196 [D loss: 0.485053, acc.: 92.19%] [G loss: 1.162964]\n",
      "197 [D loss: 0.485321, acc.: 89.06%] [G loss: 1.201806]\n",
      "198 [D loss: 0.456514, acc.: 93.75%] [G loss: 1.154240]\n",
      "199 [D loss: 0.472430, acc.: 92.19%] [G loss: 1.147551]\n",
      "200 [D loss: 0.479015, acc.: 90.62%] [G loss: 1.168567]\n",
      "201 [D loss: 0.466992, acc.: 95.31%] [G loss: 1.187257]\n",
      "202 [D loss: 0.460071, acc.: 95.31%] [G loss: 1.219842]\n",
      "203 [D loss: 0.475328, acc.: 90.62%] [G loss: 1.210512]\n",
      "204 [D loss: 0.464758, acc.: 92.19%] [G loss: 1.202485]\n",
      "205 [D loss: 0.457487, acc.: 93.75%] [G loss: 1.174640]\n",
      "206 [D loss: 0.451673, acc.: 93.75%] [G loss: 1.224566]\n",
      "207 [D loss: 0.440901, acc.: 96.88%] [G loss: 1.202413]\n",
      "208 [D loss: 0.443235, acc.: 92.19%] [G loss: 1.227067]\n",
      "209 [D loss: 0.446697, acc.: 92.19%] [G loss: 1.203696]\n",
      "210 [D loss: 0.423614, acc.: 96.88%] [G loss: 1.214752]\n",
      "211 [D loss: 0.449030, acc.: 93.75%] [G loss: 1.211306]\n",
      "212 [D loss: 0.446012, acc.: 93.75%] [G loss: 1.241771]\n",
      "213 [D loss: 0.432519, acc.: 93.75%] [G loss: 1.232471]\n",
      "214 [D loss: 0.470770, acc.: 85.94%] [G loss: 1.205622]\n",
      "215 [D loss: 0.453174, acc.: 93.75%] [G loss: 1.225374]\n",
      "216 [D loss: 0.442257, acc.: 93.75%] [G loss: 1.245588]\n",
      "217 [D loss: 0.446248, acc.: 92.19%] [G loss: 1.206800]\n",
      "218 [D loss: 0.454576, acc.: 90.62%] [G loss: 1.219350]\n",
      "219 [D loss: 0.441544, acc.: 93.75%] [G loss: 1.240775]\n",
      "220 [D loss: 0.434923, acc.: 93.75%] [G loss: 1.265500]\n",
      "221 [D loss: 0.442851, acc.: 95.31%] [G loss: 1.247833]\n",
      "222 [D loss: 0.434472, acc.: 96.88%] [G loss: 1.274809]\n",
      "223 [D loss: 0.420798, acc.: 96.88%] [G loss: 1.245872]\n",
      "224 [D loss: 0.455253, acc.: 93.75%] [G loss: 1.282781]\n",
      "225 [D loss: 0.417201, acc.: 95.31%] [G loss: 1.264083]\n",
      "226 [D loss: 0.431975, acc.: 96.88%] [G loss: 1.291022]\n",
      "227 [D loss: 0.433150, acc.: 96.88%] [G loss: 1.270415]\n",
      "228 [D loss: 0.436383, acc.: 92.19%] [G loss: 1.263008]\n",
      "229 [D loss: 0.420912, acc.: 92.19%] [G loss: 1.224833]\n",
      "230 [D loss: 0.422129, acc.: 95.31%] [G loss: 1.297111]\n",
      "231 [D loss: 0.415459, acc.: 90.62%] [G loss: 1.280179]\n",
      "232 [D loss: 0.412063, acc.: 98.44%] [G loss: 1.282825]\n",
      "233 [D loss: 0.432045, acc.: 92.19%] [G loss: 1.288979]\n",
      "234 [D loss: 0.402302, acc.: 98.44%] [G loss: 1.286378]\n",
      "235 [D loss: 0.424185, acc.: 92.19%] [G loss: 1.263444]\n",
      "236 [D loss: 0.415463, acc.: 98.44%] [G loss: 1.294088]\n",
      "237 [D loss: 0.416097, acc.: 95.31%] [G loss: 1.341061]\n",
      "238 [D loss: 0.406147, acc.: 93.75%] [G loss: 1.300883]\n",
      "239 [D loss: 0.386754, acc.: 96.88%] [G loss: 1.291454]\n",
      "240 [D loss: 0.389744, acc.: 98.44%] [G loss: 1.268330]\n",
      "241 [D loss: 0.398049, acc.: 96.88%] [G loss: 1.289114]\n",
      "242 [D loss: 0.393303, acc.: 100.00%] [G loss: 1.313735]\n",
      "243 [D loss: 0.404881, acc.: 96.88%] [G loss: 1.325431]\n",
      "244 [D loss: 0.391791, acc.: 100.00%] [G loss: 1.302746]\n",
      "245 [D loss: 0.387184, acc.: 100.00%] [G loss: 1.272081]\n",
      "246 [D loss: 0.402910, acc.: 100.00%] [G loss: 1.331107]\n",
      "247 [D loss: 0.373315, acc.: 96.88%] [G loss: 1.311654]\n",
      "248 [D loss: 0.373903, acc.: 96.88%] [G loss: 1.324592]\n",
      "249 [D loss: 0.388940, acc.: 98.44%] [G loss: 1.308741]\n",
      "250 [D loss: 0.391574, acc.: 98.44%] [G loss: 1.309352]\n",
      "251 [D loss: 0.394668, acc.: 96.88%] [G loss: 1.303401]\n",
      "252 [D loss: 0.389830, acc.: 96.88%] [G loss: 1.288026]\n",
      "253 [D loss: 0.400811, acc.: 98.44%] [G loss: 1.342561]\n",
      "254 [D loss: 0.403615, acc.: 96.88%] [G loss: 1.337258]\n",
      "255 [D loss: 0.354058, acc.: 100.00%] [G loss: 1.349375]\n",
      "256 [D loss: 0.381243, acc.: 96.88%] [G loss: 1.365885]\n",
      "257 [D loss: 0.379942, acc.: 100.00%] [G loss: 1.356915]\n",
      "258 [D loss: 0.373948, acc.: 96.88%] [G loss: 1.326126]\n",
      "259 [D loss: 0.380730, acc.: 98.44%] [G loss: 1.362905]\n",
      "260 [D loss: 0.381732, acc.: 98.44%] [G loss: 1.320844]\n",
      "261 [D loss: 0.376740, acc.: 98.44%] [G loss: 1.307251]\n",
      "262 [D loss: 0.375954, acc.: 96.88%] [G loss: 1.350407]\n",
      "263 [D loss: 0.370140, acc.: 96.88%] [G loss: 1.346701]\n",
      "264 [D loss: 0.351395, acc.: 98.44%] [G loss: 1.332367]\n",
      "265 [D loss: 0.350860, acc.: 100.00%] [G loss: 1.342516]\n",
      "266 [D loss: 0.360208, acc.: 96.88%] [G loss: 1.374014]\n",
      "267 [D loss: 0.383990, acc.: 96.88%] [G loss: 1.332215]\n",
      "268 [D loss: 0.356753, acc.: 98.44%] [G loss: 1.318530]\n",
      "269 [D loss: 0.357724, acc.: 100.00%] [G loss: 1.349645]\n",
      "270 [D loss: 0.349975, acc.: 98.44%] [G loss: 1.369915]\n",
      "271 [D loss: 0.370598, acc.: 96.88%] [G loss: 1.354465]\n",
      "272 [D loss: 0.368904, acc.: 96.88%] [G loss: 1.340544]\n",
      "273 [D loss: 0.359569, acc.: 100.00%] [G loss: 1.337174]\n",
      "274 [D loss: 0.372492, acc.: 100.00%] [G loss: 1.363697]\n",
      "275 [D loss: 0.362906, acc.: 98.44%] [G loss: 1.373123]\n",
      "276 [D loss: 0.351637, acc.: 98.44%] [G loss: 1.358853]\n",
      "277 [D loss: 0.369173, acc.: 95.31%] [G loss: 1.365381]\n",
      "278 [D loss: 0.327635, acc.: 100.00%] [G loss: 1.343541]\n",
      "279 [D loss: 0.349365, acc.: 98.44%] [G loss: 1.365966]\n",
      "280 [D loss: 0.352849, acc.: 100.00%] [G loss: 1.354793]\n",
      "281 [D loss: 0.355815, acc.: 98.44%] [G loss: 1.366204]\n",
      "282 [D loss: 0.357418, acc.: 98.44%] [G loss: 1.368647]\n",
      "283 [D loss: 0.322469, acc.: 98.44%] [G loss: 1.385803]\n",
      "284 [D loss: 0.345644, acc.: 98.44%] [G loss: 1.446764]\n",
      "285 [D loss: 0.320371, acc.: 98.44%] [G loss: 1.379126]\n",
      "286 [D loss: 0.341281, acc.: 98.44%] [G loss: 1.407052]\n",
      "287 [D loss: 0.344389, acc.: 100.00%] [G loss: 1.406600]\n",
      "288 [D loss: 0.337120, acc.: 100.00%] [G loss: 1.385655]\n",
      "289 [D loss: 0.339381, acc.: 100.00%] [G loss: 1.386595]\n",
      "290 [D loss: 0.335209, acc.: 96.88%] [G loss: 1.395835]\n",
      "291 [D loss: 0.332731, acc.: 100.00%] [G loss: 1.389130]\n",
      "292 [D loss: 0.342882, acc.: 100.00%] [G loss: 1.443565]\n",
      "293 [D loss: 0.328356, acc.: 100.00%] [G loss: 1.387809]\n",
      "294 [D loss: 0.323848, acc.: 100.00%] [G loss: 1.418416]\n",
      "295 [D loss: 0.327791, acc.: 100.00%] [G loss: 1.408880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [D loss: 0.332245, acc.: 98.44%] [G loss: 1.406744]\n",
      "297 [D loss: 0.332274, acc.: 96.88%] [G loss: 1.383331]\n",
      "298 [D loss: 0.327886, acc.: 100.00%] [G loss: 1.401032]\n",
      "299 [D loss: 0.312513, acc.: 100.00%] [G loss: 1.356863]\n",
      "300 [D loss: 0.343661, acc.: 100.00%] [G loss: 1.442144]\n",
      "301 [D loss: 0.336221, acc.: 98.44%] [G loss: 1.418436]\n",
      "302 [D loss: 0.329745, acc.: 98.44%] [G loss: 1.372619]\n",
      "303 [D loss: 0.323553, acc.: 100.00%] [G loss: 1.431607]\n",
      "304 [D loss: 0.325196, acc.: 100.00%] [G loss: 1.427466]\n",
      "305 [D loss: 0.313876, acc.: 98.44%] [G loss: 1.424736]\n",
      "306 [D loss: 0.301577, acc.: 100.00%] [G loss: 1.427947]\n",
      "307 [D loss: 0.314860, acc.: 100.00%] [G loss: 1.401195]\n",
      "308 [D loss: 0.310239, acc.: 100.00%] [G loss: 1.401670]\n",
      "309 [D loss: 0.317531, acc.: 100.00%] [G loss: 1.441594]\n",
      "310 [D loss: 0.301340, acc.: 98.44%] [G loss: 1.416257]\n",
      "311 [D loss: 0.323653, acc.: 100.00%] [G loss: 1.395576]\n",
      "312 [D loss: 0.306039, acc.: 100.00%] [G loss: 1.408870]\n",
      "313 [D loss: 0.318263, acc.: 98.44%] [G loss: 1.436105]\n",
      "314 [D loss: 0.293656, acc.: 100.00%] [G loss: 1.428969]\n",
      "315 [D loss: 0.304625, acc.: 100.00%] [G loss: 1.432930]\n",
      "316 [D loss: 0.339398, acc.: 96.88%] [G loss: 1.396088]\n",
      "317 [D loss: 0.309061, acc.: 100.00%] [G loss: 1.477219]\n",
      "318 [D loss: 0.300464, acc.: 100.00%] [G loss: 1.379522]\n",
      "319 [D loss: 0.297483, acc.: 100.00%] [G loss: 1.444015]\n",
      "320 [D loss: 0.315837, acc.: 100.00%] [G loss: 1.467355]\n",
      "321 [D loss: 0.304439, acc.: 100.00%] [G loss: 1.465730]\n",
      "322 [D loss: 0.304851, acc.: 100.00%] [G loss: 1.453613]\n",
      "323 [D loss: 0.314375, acc.: 98.44%] [G loss: 1.467317]\n",
      "324 [D loss: 0.309105, acc.: 100.00%] [G loss: 1.447793]\n",
      "325 [D loss: 0.294737, acc.: 100.00%] [G loss: 1.457448]\n",
      "326 [D loss: 0.306210, acc.: 98.44%] [G loss: 1.448374]\n",
      "327 [D loss: 0.288740, acc.: 100.00%] [G loss: 1.434721]\n",
      "328 [D loss: 0.299345, acc.: 100.00%] [G loss: 1.448358]\n",
      "329 [D loss: 0.302507, acc.: 100.00%] [G loss: 1.469680]\n",
      "330 [D loss: 0.300314, acc.: 100.00%] [G loss: 1.468393]\n",
      "331 [D loss: 0.300118, acc.: 100.00%] [G loss: 1.441310]\n",
      "332 [D loss: 0.287735, acc.: 98.44%] [G loss: 1.451049]\n",
      "333 [D loss: 0.292187, acc.: 100.00%] [G loss: 1.487923]\n",
      "334 [D loss: 0.322940, acc.: 100.00%] [G loss: 1.467202]\n",
      "335 [D loss: 0.297775, acc.: 100.00%] [G loss: 1.470884]\n",
      "336 [D loss: 0.298756, acc.: 100.00%] [G loss: 1.429140]\n",
      "337 [D loss: 0.322754, acc.: 100.00%] [G loss: 1.446551]\n",
      "338 [D loss: 0.301254, acc.: 100.00%] [G loss: 1.475479]\n",
      "339 [D loss: 0.290185, acc.: 100.00%] [G loss: 1.484679]\n",
      "340 [D loss: 0.267649, acc.: 100.00%] [G loss: 1.545856]\n",
      "341 [D loss: 0.297852, acc.: 100.00%] [G loss: 1.504068]\n",
      "342 [D loss: 0.282663, acc.: 100.00%] [G loss: 1.486993]\n",
      "343 [D loss: 0.276656, acc.: 100.00%] [G loss: 1.467452]\n",
      "344 [D loss: 0.292494, acc.: 98.44%] [G loss: 1.522664]\n",
      "345 [D loss: 0.287435, acc.: 100.00%] [G loss: 1.485670]\n",
      "346 [D loss: 0.276895, acc.: 100.00%] [G loss: 1.444874]\n",
      "347 [D loss: 0.300018, acc.: 98.44%] [G loss: 1.445434]\n",
      "348 [D loss: 0.290057, acc.: 100.00%] [G loss: 1.453592]\n",
      "349 [D loss: 0.291183, acc.: 100.00%] [G loss: 1.465730]\n",
      "350 [D loss: 0.267535, acc.: 100.00%] [G loss: 1.537013]\n",
      "351 [D loss: 0.293511, acc.: 100.00%] [G loss: 1.508565]\n",
      "352 [D loss: 0.277360, acc.: 100.00%] [G loss: 1.471564]\n",
      "353 [D loss: 0.288176, acc.: 100.00%] [G loss: 1.512562]\n",
      "354 [D loss: 0.273664, acc.: 100.00%] [G loss: 1.495511]\n",
      "355 [D loss: 0.277994, acc.: 100.00%] [G loss: 1.517205]\n",
      "356 [D loss: 0.250958, acc.: 100.00%] [G loss: 1.534020]\n",
      "357 [D loss: 0.289911, acc.: 100.00%] [G loss: 1.542305]\n",
      "358 [D loss: 0.258705, acc.: 100.00%] [G loss: 1.514597]\n",
      "359 [D loss: 0.246612, acc.: 100.00%] [G loss: 1.522446]\n",
      "360 [D loss: 0.265817, acc.: 100.00%] [G loss: 1.499031]\n",
      "361 [D loss: 0.264361, acc.: 100.00%] [G loss: 1.518759]\n",
      "362 [D loss: 0.282925, acc.: 100.00%] [G loss: 1.569690]\n",
      "363 [D loss: 0.279066, acc.: 100.00%] [G loss: 1.504856]\n",
      "364 [D loss: 0.291423, acc.: 98.44%] [G loss: 1.528866]\n",
      "365 [D loss: 0.269713, acc.: 98.44%] [G loss: 1.484979]\n",
      "366 [D loss: 0.276065, acc.: 100.00%] [G loss: 1.470647]\n",
      "367 [D loss: 0.267162, acc.: 100.00%] [G loss: 1.540058]\n",
      "368 [D loss: 0.266733, acc.: 100.00%] [G loss: 1.508467]\n",
      "369 [D loss: 0.270164, acc.: 100.00%] [G loss: 1.513184]\n",
      "370 [D loss: 0.261096, acc.: 100.00%] [G loss: 1.533242]\n",
      "371 [D loss: 0.277604, acc.: 100.00%] [G loss: 1.520646]\n",
      "372 [D loss: 0.266443, acc.: 100.00%] [G loss: 1.514028]\n",
      "373 [D loss: 0.278774, acc.: 100.00%] [G loss: 1.518412]\n",
      "374 [D loss: 0.258838, acc.: 100.00%] [G loss: 1.573248]\n",
      "375 [D loss: 0.264416, acc.: 100.00%] [G loss: 1.544336]\n",
      "376 [D loss: 0.255537, acc.: 100.00%] [G loss: 1.542639]\n",
      "377 [D loss: 0.255834, acc.: 100.00%] [G loss: 1.544903]\n",
      "378 [D loss: 0.253735, acc.: 100.00%] [G loss: 1.581872]\n",
      "379 [D loss: 0.253275, acc.: 100.00%] [G loss: 1.581157]\n",
      "380 [D loss: 0.261439, acc.: 100.00%] [G loss: 1.552676]\n",
      "381 [D loss: 0.252358, acc.: 100.00%] [G loss: 1.527020]\n",
      "382 [D loss: 0.260914, acc.: 98.44%] [G loss: 1.542740]\n",
      "383 [D loss: 0.251726, acc.: 100.00%] [G loss: 1.594656]\n",
      "384 [D loss: 0.260337, acc.: 100.00%] [G loss: 1.537017]\n",
      "385 [D loss: 0.250600, acc.: 100.00%] [G loss: 1.572802]\n",
      "386 [D loss: 0.249339, acc.: 100.00%] [G loss: 1.596430]\n",
      "387 [D loss: 0.249354, acc.: 100.00%] [G loss: 1.607332]\n",
      "388 [D loss: 0.254102, acc.: 100.00%] [G loss: 1.616039]\n",
      "389 [D loss: 0.250891, acc.: 100.00%] [G loss: 1.571530]\n",
      "390 [D loss: 0.245468, acc.: 100.00%] [G loss: 1.565625]\n",
      "391 [D loss: 0.244639, acc.: 100.00%] [G loss: 1.619275]\n",
      "392 [D loss: 0.262542, acc.: 100.00%] [G loss: 1.544472]\n",
      "393 [D loss: 0.266698, acc.: 100.00%] [G loss: 1.655138]\n",
      "394 [D loss: 0.239747, acc.: 100.00%] [G loss: 1.538166]\n",
      "395 [D loss: 0.251450, acc.: 100.00%] [G loss: 1.511199]\n",
      "396 [D loss: 0.242537, acc.: 100.00%] [G loss: 1.551924]\n",
      "397 [D loss: 0.247721, acc.: 100.00%] [G loss: 1.606866]\n",
      "398 [D loss: 0.263596, acc.: 98.44%] [G loss: 1.585344]\n",
      "399 [D loss: 0.246193, acc.: 100.00%] [G loss: 1.648220]\n",
      "400 [D loss: 0.245959, acc.: 98.44%] [G loss: 1.588070]\n",
      "401 [D loss: 0.226670, acc.: 100.00%] [G loss: 1.631520]\n",
      "402 [D loss: 0.251170, acc.: 100.00%] [G loss: 1.595069]\n",
      "403 [D loss: 0.263001, acc.: 98.44%] [G loss: 1.594669]\n",
      "404 [D loss: 0.256387, acc.: 100.00%] [G loss: 1.641459]\n",
      "405 [D loss: 0.230673, acc.: 100.00%] [G loss: 1.675203]\n",
      "406 [D loss: 0.252743, acc.: 100.00%] [G loss: 1.562779]\n",
      "407 [D loss: 0.242893, acc.: 98.44%] [G loss: 1.586874]\n",
      "408 [D loss: 0.257668, acc.: 100.00%] [G loss: 1.601103]\n",
      "409 [D loss: 0.238107, acc.: 100.00%] [G loss: 1.659190]\n",
      "410 [D loss: 0.232310, acc.: 100.00%] [G loss: 1.584510]\n",
      "411 [D loss: 0.243083, acc.: 100.00%] [G loss: 1.634674]\n",
      "412 [D loss: 0.248724, acc.: 100.00%] [G loss: 1.711179]\n",
      "413 [D loss: 0.241501, acc.: 100.00%] [G loss: 1.619271]\n",
      "414 [D loss: 0.232945, acc.: 100.00%] [G loss: 1.610423]\n",
      "415 [D loss: 0.216423, acc.: 100.00%] [G loss: 1.613339]\n",
      "416 [D loss: 0.247957, acc.: 100.00%] [G loss: 1.590193]\n",
      "417 [D loss: 0.255815, acc.: 98.44%] [G loss: 1.652510]\n",
      "418 [D loss: 0.218440, acc.: 100.00%] [G loss: 1.676359]\n",
      "419 [D loss: 0.228385, acc.: 100.00%] [G loss: 1.614934]\n",
      "420 [D loss: 0.219972, acc.: 100.00%] [G loss: 1.662989]\n",
      "421 [D loss: 0.234654, acc.: 100.00%] [G loss: 1.641220]\n",
      "422 [D loss: 0.235297, acc.: 100.00%] [G loss: 1.644057]\n",
      "423 [D loss: 0.249797, acc.: 98.44%] [G loss: 1.664610]\n",
      "424 [D loss: 0.226283, acc.: 100.00%] [G loss: 1.658866]\n",
      "425 [D loss: 0.228344, acc.: 100.00%] [G loss: 1.611153]\n",
      "426 [D loss: 0.248412, acc.: 100.00%] [G loss: 1.685623]\n",
      "427 [D loss: 0.231628, acc.: 100.00%] [G loss: 1.630090]\n",
      "428 [D loss: 0.234793, acc.: 100.00%] [G loss: 1.634887]\n",
      "429 [D loss: 0.235399, acc.: 100.00%] [G loss: 1.602839]\n",
      "430 [D loss: 0.219896, acc.: 100.00%] [G loss: 1.730401]\n",
      "431 [D loss: 0.249551, acc.: 100.00%] [G loss: 1.601815]\n",
      "432 [D loss: 0.244202, acc.: 100.00%] [G loss: 1.711211]\n",
      "433 [D loss: 0.228439, acc.: 100.00%] [G loss: 1.668105]\n",
      "434 [D loss: 0.229412, acc.: 100.00%] [G loss: 1.715756]\n",
      "435 [D loss: 0.238605, acc.: 100.00%] [G loss: 1.653923]\n",
      "436 [D loss: 0.223843, acc.: 100.00%] [G loss: 1.622991]\n",
      "437 [D loss: 0.230219, acc.: 100.00%] [G loss: 1.664115]\n",
      "438 [D loss: 0.223296, acc.: 100.00%] [G loss: 1.673546]\n",
      "439 [D loss: 0.228162, acc.: 100.00%] [G loss: 1.642115]\n",
      "440 [D loss: 0.228450, acc.: 100.00%] [G loss: 1.690624]\n",
      "441 [D loss: 0.228499, acc.: 100.00%] [G loss: 1.673403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 [D loss: 0.238572, acc.: 100.00%] [G loss: 1.669432]\n",
      "443 [D loss: 0.235933, acc.: 100.00%] [G loss: 1.640665]\n",
      "444 [D loss: 0.223380, acc.: 100.00%] [G loss: 1.677243]\n",
      "445 [D loss: 0.228304, acc.: 100.00%] [G loss: 1.673677]\n",
      "446 [D loss: 0.224245, acc.: 100.00%] [G loss: 1.688083]\n",
      "447 [D loss: 0.225312, acc.: 100.00%] [G loss: 1.700754]\n",
      "448 [D loss: 0.219684, acc.: 100.00%] [G loss: 1.728777]\n",
      "449 [D loss: 0.206313, acc.: 100.00%] [G loss: 1.710760]\n",
      "450 [D loss: 0.220859, acc.: 98.44%] [G loss: 1.654894]\n",
      "451 [D loss: 0.220799, acc.: 100.00%] [G loss: 1.680372]\n",
      "452 [D loss: 0.213736, acc.: 100.00%] [G loss: 1.732551]\n",
      "453 [D loss: 0.219635, acc.: 100.00%] [G loss: 1.706493]\n",
      "454 [D loss: 0.218227, acc.: 100.00%] [G loss: 1.722857]\n",
      "455 [D loss: 0.225999, acc.: 100.00%] [G loss: 1.666876]\n",
      "456 [D loss: 0.231245, acc.: 100.00%] [G loss: 1.649857]\n",
      "457 [D loss: 0.215242, acc.: 100.00%] [G loss: 1.716781]\n",
      "458 [D loss: 0.216845, acc.: 100.00%] [G loss: 1.762435]\n",
      "459 [D loss: 0.217196, acc.: 100.00%] [G loss: 1.691289]\n",
      "460 [D loss: 0.218106, acc.: 100.00%] [G loss: 1.684134]\n",
      "461 [D loss: 0.213902, acc.: 100.00%] [G loss: 1.723797]\n",
      "462 [D loss: 0.201822, acc.: 100.00%] [G loss: 1.741978]\n",
      "463 [D loss: 0.212470, acc.: 100.00%] [G loss: 1.683037]\n",
      "464 [D loss: 0.225091, acc.: 100.00%] [G loss: 1.669970]\n",
      "465 [D loss: 0.208189, acc.: 100.00%] [G loss: 1.724233]\n",
      "466 [D loss: 0.206368, acc.: 100.00%] [G loss: 1.764100]\n",
      "467 [D loss: 0.199577, acc.: 100.00%] [G loss: 1.759780]\n",
      "468 [D loss: 0.221778, acc.: 100.00%] [G loss: 1.724369]\n",
      "469 [D loss: 0.205812, acc.: 100.00%] [G loss: 1.696031]\n",
      "470 [D loss: 0.220351, acc.: 100.00%] [G loss: 1.725192]\n",
      "471 [D loss: 0.228299, acc.: 100.00%] [G loss: 1.743645]\n",
      "472 [D loss: 0.223291, acc.: 98.44%] [G loss: 1.672556]\n",
      "473 [D loss: 0.208389, acc.: 100.00%] [G loss: 1.656018]\n",
      "474 [D loss: 0.201559, acc.: 100.00%] [G loss: 1.716143]\n",
      "475 [D loss: 0.197076, acc.: 100.00%] [G loss: 1.658136]\n",
      "476 [D loss: 0.219542, acc.: 100.00%] [G loss: 1.747115]\n",
      "477 [D loss: 0.201936, acc.: 100.00%] [G loss: 1.706098]\n",
      "478 [D loss: 0.219816, acc.: 100.00%] [G loss: 1.665445]\n",
      "479 [D loss: 0.203910, acc.: 100.00%] [G loss: 1.794267]\n",
      "480 [D loss: 0.217477, acc.: 100.00%] [G loss: 1.774103]\n",
      "481 [D loss: 0.205853, acc.: 100.00%] [G loss: 1.713475]\n",
      "482 [D loss: 0.202816, acc.: 100.00%] [G loss: 1.747381]\n",
      "483 [D loss: 0.206340, acc.: 100.00%] [G loss: 1.732934]\n",
      "484 [D loss: 0.198064, acc.: 100.00%] [G loss: 1.689849]\n",
      "485 [D loss: 0.213614, acc.: 100.00%] [G loss: 1.810290]\n",
      "486 [D loss: 0.192217, acc.: 100.00%] [G loss: 1.745095]\n",
      "487 [D loss: 0.209465, acc.: 100.00%] [G loss: 1.758471]\n",
      "488 [D loss: 0.197367, acc.: 100.00%] [G loss: 1.766932]\n",
      "489 [D loss: 0.195721, acc.: 100.00%] [G loss: 1.732024]\n",
      "490 [D loss: 0.224717, acc.: 100.00%] [G loss: 1.780664]\n",
      "491 [D loss: 0.194511, acc.: 100.00%] [G loss: 1.716730]\n",
      "492 [D loss: 0.196035, acc.: 100.00%] [G loss: 1.802923]\n",
      "493 [D loss: 0.199438, acc.: 100.00%] [G loss: 1.755735]\n",
      "494 [D loss: 0.200550, acc.: 100.00%] [G loss: 1.712661]\n",
      "495 [D loss: 0.206911, acc.: 100.00%] [G loss: 1.808097]\n",
      "496 [D loss: 0.195135, acc.: 100.00%] [G loss: 1.731794]\n",
      "497 [D loss: 0.210245, acc.: 100.00%] [G loss: 1.750058]\n",
      "498 [D loss: 0.191630, acc.: 100.00%] [G loss: 1.719891]\n",
      "499 [D loss: 0.217292, acc.: 100.00%] [G loss: 1.762712]\n",
      "500 [D loss: 0.208178, acc.: 100.00%] [G loss: 1.747775]\n",
      "501 [D loss: 0.191697, acc.: 100.00%] [G loss: 1.688373]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-31275a60f4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/06147/pberger/.virtualenvs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/06147/pberger/.virtualenvs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/06147/pberger/.virtualenvs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/06147/pberger/.virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the darn thing\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "            \n",
    "    # Generate some true images\n",
    "    imgs = np.zeros((batch_size, img_rows, img_cols))\n",
    "    for i in range(batch_size):\n",
    "        imgs[i] = grf_2d(1.0, alpha=-2.0)\n",
    "    imgs =imgs.reshape((batch_size, img_rows, img_cols, 1))\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    discriminator.trainable = True\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Train the generator (to have the discriminator label samples as valid)\n",
    "    discriminator.trainable = False\n",
    "    g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "    # Plot the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    #if epoch % sample_interval == 0:\n",
    "    #    self.sample_images(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
